---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "80%"
)
```

# Evaluating Probabilistic Forecasts for Maritime Engineering Operations

<!-- badges: start -->
<!-- badges: end -->

This is the GitHub repo for the translational paper ''Evaluating Probabilistic Forecasts for Maritime Engineering Operations'' submitted to *Data-centric Engineering*. The main purpose of the paper is to make existing statistical methodology available to an engineering audience with the hope that it may be more widely adopted. The methodology is then supported by a case study of forecasting surface winds at a location in Australia's North--West Shelf. This repository is broken into two sections: the first, and main section, shows code and methods for calculating three chosen scoring rules given a probabilistic forecast; the second section documents the code for the case study in the paper. Some data is provided alongside. Note, we heavily use [tidyverse](https://www.tidyverse.org) packages and syntax; in particular, [pipes](https://style.tidyverse.org/pipes.html).

## Scoring Rules

To fully assess a probabilistic forecast we advocate three scoring rules: the squared error (SE); the Dawid-Sebastiani score (DSS); and the continuous ranked probability score (CRPS) or its multivariate analogue the energy score (ES). Roughly, the SE assesses the forecast mean, the DSS assesses the forecast tails, and the CRPS/ES assesses the main body of the forecast. Definitions and discussions are in the paper. First we introduce some data and the forecasts of a probabilistic forecasting model, then we show how the scores may be calculated.

### Data and Forecasting Output

We use the predictions from the Model Output Statistics (MOS) model made at 00:00:00AWST 06-07-2018 (this is the same time as shown in Figure 3 of the paper). The MOS model is a Bayesian forecasting model estimated via Markov chain Monte Carlo (MCMC). As such, parametric forecast distributions are not available at each prediction horizon; rather, we have a collection of samples. The tibble (the particular data structure we use to store the forecasts) of the forecasts is shown below; it documents the time issued (*t*), the time forecast (*t+h*), the prediction horizon (*h*), the sample from the forecasting model (*j*), the true measured eastings and northings at the time forecast, and the forecast eastings and northings.

```{r, warning = FALSE, message = FALSE}
library(tidyverse)

mos_predictions <- readRDS("data/mos_predictions.RDS")
mos_predictions

```

### Calculating Scoring Rules

**Squared error** calculates the distance between the mean of the forecast distribution and the observed values. The forecast mean is approximated by the sample mean and the SE is calculated at each prediction horzion as per Equation 3.1.

```{r, warning = FALSE, message = FALSE}
mos_mean <- mos_predictions %>%
  group_by(time_issued, time_forecast, meas_east, meas_north) %>%
  summarise(
    mu_east = mean(fore_east),
    mu_north = mean(fore_north)
  ) %>%
  ungroup()

# The forecast means
mos_mean

# The SEs for each prediction horizon
mos_mean %>%
  mutate(
    SE = (meas_east - mu_east)^2 + (meas_north - mu_north)^2
  )
```

**The Dawid-Sebastiani Score** requires the forecast means and variances. It requires slightly more calculation (due to estimating a sample variance matrix). First, we define the DSS loss function. Then we use <tt>lapply</tt> to loop over each prediction horizon and calculate DSS.

```{r, warning = FALSE, message = FALSE}

# DSS loss function
calc_DSS <- function(obs, mu, Sigma){
  ld <- log(det(Sigma))
  DSS <- ld + t(mu - obs) %*% solve(Sigma) %*% (mu - obs)
  return(as.numeric(DSS))
}

# Define the prediction horizons to loop over (this is the prediction domain)
horizons <- mos_predictions$horizon %>% unique()

lapply(horizons, function(i){
  
  # Subset the predictions at h
  mos_h <- mos_predictions %>%
  filter(horizon == i)

  # Define the observed value
  obs_h <- as.matrix(c(mos_h$meas_east[1], mos_h$meas_north[1]))

  # Collate the forecasts into a matrix
  forecasts_h <- matrix(
    c(mos_h$fore_east, mos_h$fore_north), 
    ncol = 2
  )

  # Calculate the empirical mean and variance
  fmean <- colMeans(forecasts_h) %>% matrix()
  fvar <- var(forecasts_h)

  # Calculate DSS
  DSS_calc <- calc_DSS(obs_h, fmean, fvar)

  #  Collate back into a tibble and return
  mos_h[1,] %>%
    dplyr::select(time_issued, time_forecast, horizon) %>%
    mutate(DSS = DSS_calc)
}) %>%
  bind_rows()

```

**The continuous ranked probability score** and the **energy score** require the cumulative distribution function (CDF) of the forecast. When forecasts are described via samples, Equations 3.4 and 3.6 can be used to instead calculate CRPS and ES, respectively. In <tt>R</tt>, the <tt>scoringRules</tt> package may be used to efficiently calculate this via either <tt>crps_sample</tt> or <tt>es_sample</tt>. Note, similar packages exist in [python](https://pypi.org/project/properscoring/) and [MatLab](https://www.mathworks.com/matlabcentral/fileexchange/77203-acps-package).

```{r}
library(scoringRules)

lapply(horizons, function(i){
  
  # Subset the predictions at h
  mos_h <- mos_predictions %>%
  filter(horizon == i)

  # Define the observed value
  obs_h <- c(mos_h$meas_east[1], mos_h$meas_north[1])

  # Collate the forecasts into a matrix
  forecasts_h <- matrix(
    c(mos_h$fore_east, mos_h$fore_north), 
    ncol = 2
  )

  # Calculate ES
  ES_calc <- es_sample(obs_h, t(forecasts_h))

  #  Collate back into a tibble and return
  mos_h[1,] %>%
    dplyr::select(time_issued, time_forecast, horizon) %>%
    mutate(ES = ES_calc)
}) %>%
  bind_rows()

```

## Case Study of Surface Wind Prediction

We 




